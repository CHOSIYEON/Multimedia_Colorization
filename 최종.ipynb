{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "최종",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHOSIYEON/Multimedia_Colorization/blob/main/%EC%B5%9C%EC%A2%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3taJpW8YSMfA"
      },
      "source": [
        "# Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75_biZViSQS_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuHRHQhGSYiE"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tqdm\n",
        "\n",
        "file_name = \"Multimedia_dataset.zip\"\n",
        "zip_path = os.path.join('/content/drive/MyDrive/Multimedia_dataset.zip')\n",
        "\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q \"{file_name}\"\n",
        "!rm \"{file_name}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R1gU3zOWdM6"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tqdm\n",
        "\n",
        "file_name = \"colorization_test_dataset.zip\"\n",
        "zip_path = os.path.join('/content/drive/MyDrive/colorization_test_dataset.zip')\n",
        "\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q \"{file_name}\"\n",
        "!rm \"{file_name}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhrgzguZSisl"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_root = './train'\n",
        "val_root = './validation'\n",
        "\n",
        "train_examples = os.listdir(train_root)\n",
        "val_examples = os.listdir(val_root)\n",
        "\n",
        "print(len(train_examples)) #4500\n",
        "print(len(val_examples)) # 500\n",
        "\n",
        "# image read\n",
        "img = plt.imread(train_root + '/' + train_examples[1])\n",
        "# image show\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HidhQqvYS2SR"
      },
      "source": [
        "# Color-hint Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2g_HC2YS5KF"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class ColorHintTransform(object):\n",
        "  def __init__(self, size=256, mode=\"training\"):\n",
        "    super(ColorHintTransform, self).__init__()\n",
        "    self.size = size\n",
        "    self.mode = mode\n",
        "    self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "  def bgr_to_lab(self, img):\n",
        "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "    l, ab = lab[:, :, 0], lab[:, :, 1:]\n",
        "    return l, ab\n",
        "\n",
        "  def hint_mask(self, bgr, threshold=[0.95, 0.97, 0.99]):\n",
        "    h, w, c = bgr.shape\n",
        "    mask_threshold = random.choice(threshold)\n",
        "    mask = np.random.random([h, w, 1]) > mask_threshold\n",
        "    return mask\n",
        "\n",
        "  def img_to_mask(self, mask_img):\n",
        "    mask = mask_img[:, :, 0, np.newaxis] >= 255\n",
        "    return mask\n",
        "\n",
        "  def __call__(self, img, mask_img=None):\n",
        "    threshold = [0.95, 0.97, 0.99]\n",
        "    if (self.mode == \"training\") | (self.mode == \"validation\"):\n",
        "      image = cv2.resize(img, (self.size, self.size))\n",
        "      mask = self.hint_mask(image, threshold)\n",
        "\n",
        "      hint_image = image * mask\n",
        "\n",
        "      l, ab = self.bgr_to_lab(image)\n",
        "      l_hint, ab_hint = self.bgr_to_lab(hint_image)\n",
        "\n",
        "      return self.transform(l), self.transform(ab), self.transform(ab_hint)\n",
        "\n",
        "    elif self.mode == \"testing\":\n",
        "      image = cv2.resize(img, (self.size, self.size))\n",
        "      hint_image = image * self.img_to_mask(mask_img)\n",
        "\n",
        "      l, _ = self.bgr_to_lab(image)\n",
        "      _, ab_hint = self.bgr_to_lab(hint_image)\n",
        "\n",
        "      return self.transform(l), self.transform(ab_hint)\n",
        "\n",
        "    else:\n",
        "      return NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-xD0pAS-dq"
      },
      "source": [
        "# Dataloader for Colorization Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3FKb2qsTBvS"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "class ColorHintDataset(data.Dataset):\n",
        "  def __init__(self, root_path, size):\n",
        "    super(ColorHintDataset, self).__init__()\n",
        "\n",
        "    self.root_path = root_path\n",
        "    self.size = size\n",
        "    self.transforms = None\n",
        "    self.examples = None\n",
        "    self.hint = None\n",
        "    self.mask = None\n",
        "\n",
        "  def set_mode(self, mode):\n",
        "    self.mode = mode\n",
        "    self.transforms = ColorHintTransform(self.size, mode)\n",
        "    if mode == \"training\":\n",
        "      train_dir = os.path.join(self.root_path, \"train\")\n",
        "      self.examples = [os.path.join(self.root_path, \"train\", dirs) for dirs in os.listdir(train_dir)]\n",
        "    elif mode == \"validation\":\n",
        "      val_dir = os.path.join(self.root_path, \"validation\")\n",
        "      self.examples = [os.path.join(self.root_path, \"validation\", dirs) for dirs in os.listdir(val_dir)]\n",
        "    elif mode == \"testing\":\n",
        "      hint_dir = os.path.join(self.root_path, \"hint\")\n",
        "      mask_dir = os.path.join(self.root_path, \"mask\")\n",
        "      self.hint = [os.path.join(self.root_path, \"hint\", dirs) for dirs in os.listdir(hint_dir)]\n",
        "      self.mask = [os.path.join(self.root_path, \"mask\", dirs) for dirs in os.listdir(mask_dir)]\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.mode != \"testing\":\n",
        "      return len(self.examples)\n",
        "    else:\n",
        "      return len(self.hint)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.mode == \"testing\":\n",
        "      hint_file_name = self.hint[idx]\n",
        "      mask_file_name = self.mask[idx]\n",
        "      hint_img = cv2.imread(hint_file_name)\n",
        "      mask_img = cv2.imread(mask_file_name)\n",
        "\n",
        "      input_l, input_hint = self.transforms(hint_img, mask_img)\n",
        "      sample = {\"l\": input_l, \"hint\": input_hint,\n",
        "                \"file_name\": \"image_%06d.png\" % int(os.path.basename(hint_file_name).split('.')[0])}\n",
        "    else:\n",
        "      file_name = self.examples[idx]\n",
        "      img = cv2.imread(file_name)\n",
        "      l, ab, hint = self.transforms(img)\n",
        "      sample = {\"l\": l, \"ab\": ab, \"hint\": hint}\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NK7q41si_5J"
      },
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT10xlc9TFjk"
      },
      "source": [
        "# Example for Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWiEeaWlTI6a"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision.transforms import Compose, ToTensor, ToPILImage\n",
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "  if isinstance(input_image, torch.Tensor):\n",
        "      image_tensor = input_image.data\n",
        "  else:\n",
        "      return input_image\n",
        "  image_numpy = image_tensor[0].cpu().float().numpy()\n",
        "  if image_numpy.shape[0] == 1:\n",
        "      image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "  image_numpy = np.clip((np.transpose(image_numpy, (1, 2, 0)) ),0, 1) * 255.0\n",
        "  return image_numpy.astype(imtype)\n",
        "\n",
        "# Change to your data root directory\n",
        "root_path = \"/content/\"\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "train_dataset = ColorHintDataset(root_path, 128)\n",
        "train_dataset.set_mode(\"training\")\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "val_dataset = ColorHintDataset(root_path, 128)\n",
        "val_dataset.set_mode(\"validation\")\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "test_dataset = ColorHintDataset(root_path, 128)\n",
        "test_dataset.set_mode(\"testing\")\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "# for i, data in enumerate(tqdm.tqdm(train_dataloader)):\n",
        "#   if use_cuda:\n",
        "#     l = data[\"l\"].to('cuda')\n",
        "#     ab = data[\"ab\"].to('cuda')\n",
        "#     hint = data[\"hint\"].to('cuda')\n",
        "  \n",
        "#   gt_image = torch.cat((l, ab), dim=1)\n",
        "#   hint_image = torch.cat((l, hint), dim=1)\n",
        "\n",
        "#   gt_np = tensor2im(gt_image)\n",
        "#   hint_np = tensor2im(hint_image)\n",
        "\n",
        "#   gt_bgr = cv2.cvtColor(gt_np, cv2.COLOR_LAB2BGR)\n",
        "#   hint_bgr = cv2.cvtColor(hint_np, cv2.COLOR_LAB2BGR)\n",
        "  \n",
        "  # cv2_imshow(gt_bgr)\n",
        "  # cv2_imshow(hint_bgr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuEKjx-kTso3"
      },
      "source": [
        "# Network Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugkpu7_NyhEw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(UNet, self).__init__()\n",
        "\n",
        "    def conv(in_channel, out_channel):\n",
        "      layers = []\n",
        "      layers += [nn.Conv2d(in_channels = in_channel, out_channels = out_channel, kernel_size = 3, stride = 1, padding = 1,\n",
        "                           bias = True)]\n",
        "      layers += [nn.BatchNorm2d(num_features = out_channel)]\n",
        "      layers += [nn.ReLU()]\n",
        "\n",
        "      model = nn.Sequential(*layers)\n",
        "      return model\n",
        "\n",
        "    def pool():\n",
        "      model = nn.MaxPool2d(kernel_size=2)\n",
        "      return model \n",
        "\n",
        "    def unpool(in_channel, out_channel):\n",
        "      model = nn.ConvTranspose2d(in_channels = in_channel, out_channels = out_channel, kernel_size = 2, stride = 2, \n",
        "                                 padding = 0, bias = True)\n",
        "      return model\n",
        "\n",
        "    # Contracting path \n",
        "    self.enc1 = conv(3, 32)\n",
        "    self.enc1_ = conv(32, 32)\n",
        "    self.pool1 = pool()\n",
        "\n",
        "    self.enc2 = conv(32, 64)\n",
        "    self.enc2_ = conv(64, 64)\n",
        "    self.pool2 = pool()\n",
        "\n",
        "    self.enc3 = conv(64, 128)\n",
        "    self.enc3_ = conv(128, 128)\n",
        "    self.pool3 = pool()\n",
        "\n",
        "    self.enc4 = conv(128, 256)\n",
        "    self.enc4_ = conv(256, 256)\n",
        "    self.pool4 = pool()\n",
        "\n",
        "    self.enc5 = conv(256, 512)\n",
        "    self.enc5_ = conv(512, 512)\n",
        "    self.pool5 = pool()\n",
        "\n",
        "    self.enc6 = conv(512, 1024)\n",
        "\n",
        "    # Expanding path\n",
        "    self.dec6 = conv(1024, 512)\n",
        "    self.unpool5 = unpool(512, 512)\n",
        "\n",
        "    self.dec5_ = conv(1024, 512)\n",
        "    self.dec5 = conv(512, 256)\n",
        "    self.unpool4 = unpool(256, 256)\n",
        "\n",
        "    self.dec4_ = conv(512, 256)\n",
        "    self.dec4 = conv(256, 128)\n",
        "    self.unpool3 = unpool(128, 128)\n",
        "\n",
        "    self.dec3_ = conv(256, 128)\n",
        "    self.dec3 = conv(128, 64)\n",
        "    self.unpool2 = unpool(64, 64)\n",
        "\n",
        "    self.dec2_ = conv(128, 64)\n",
        "    self.dec2 = conv(64, 32)\n",
        "    self.unpool1 = unpool(32, 32)\n",
        "\n",
        "    self.dec1_ = conv(64, 32)\n",
        "    self.dec1 = conv(32, 32)\n",
        "    \n",
        "    self.fc = nn.Conv2d(in_channels=32, out_channels=2, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    enc1 = F.relu(self.enc1(x))\n",
        "    enc1 = F.relu(self.enc1_(enc1))\n",
        "    pool1 = F.relu(self.pool1(enc1))\n",
        "\n",
        "    enc2 = F.relu(self.enc2(pool1))\n",
        "    enc2 = F.relu(self.enc2_(enc2))\n",
        "    pool2 = F.relu(self.pool2(enc2))\n",
        "\n",
        "    enc3 = F.relu(self.enc3(pool2))\n",
        "    enc3 = F.relu(self.enc3_(enc3))\n",
        "    pool3 = F.relu(self.pool3(enc3))\n",
        "\n",
        "    enc4 = F.relu(self.enc4(pool3))\n",
        "    enc4 = F.relu(self.enc4_(enc4))\n",
        "    pool4 = F.relu(self.pool4(enc4))\n",
        "\n",
        "    enc5 = F.relu(self.enc5(pool4))\n",
        "    enc5 = F.relu(self.enc5_(enc5))\n",
        "    pool5 = F.relu(self.pool5(enc5))\n",
        "\n",
        "    enc6 = F.relu(self.enc6(pool5))\n",
        "\n",
        "    dec6 = F.relu(self.dec6(enc6))\n",
        "\n",
        "    unpool5 = F.relu(self.unpool5(dec6))\n",
        "    merge5 = torch.cat((unpool5, enc5), dim = 1)\n",
        "    dec5 = F.relu(self.dec5_(merge5))\n",
        "    dec5 = F.relu(self.dec5(dec5))\n",
        "\n",
        "    unpool4 = F.relu(self.unpool4(dec5))\n",
        "    merge4 = torch.cat((unpool4, enc4), dim = 1)\n",
        "    dec4 = F.relu(self.dec4_(merge4))\n",
        "    dec4 = F.relu(self.dec4(dec4))\n",
        "\n",
        "    unpool3 = F.relu(self.unpool3(dec4))\n",
        "    merge3 = torch.cat((unpool3, enc3), dim = 1)\n",
        "    dec3 = F.relu(self.dec3_(merge3))\n",
        "    dec3 = F.relu(self.dec3(dec3))\n",
        "\n",
        "    unpool2 = F.relu(self.unpool2(dec3))\n",
        "    merge2 = torch.cat((unpool2, enc2), dim = 1)\n",
        "    dec2 = F.relu(self.dec2_(merge2))\n",
        "    dec2 = F.relu(self.dec2(dec2))\n",
        "\n",
        "    unpool1 = F.relu(self.unpool1(dec2))\n",
        "    merge1 = torch.cat((unpool1, enc1), dim = 1)\n",
        "    dec1 = F.relu(self.dec1_(merge1))\n",
        "    dec1 = F.relu(self.dec1(dec1))\n",
        "\n",
        "    out = self.fc(dec1)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQmUwLKpaVx8"
      },
      "source": [
        "import time\n",
        "\n",
        "net = UNet()\n",
        "s = time.time()\n",
        "\n",
        "for i in range(3):\n",
        "  dummy_input = torch.rand(50, 3, 128, 128)\n",
        "  out = net(dummy_input)\n",
        "  # output_np = tensor2im(out)\n",
        "  # output_bgr = cv2.cvtColor(output_np, cv2.COLOR_LAB2BGR)\n",
        "  # cv2_imshow(output_bgr)\n",
        "  print(out.shape)\n",
        "\n",
        "e = time.time()\n",
        "\n",
        "print(e-s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkEqaeC7dNc7"
      },
      "source": [
        "# Network Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwt6MZc_dP7N"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print('train dataset length: ', len(train_dataloader))\n",
        "print('validation dataset length: ', len(val_dataloader))\n",
        "\n",
        "# 1. Network Setting\n",
        "net = UNet().cuda()\n",
        "\n",
        "# 2. Loss ans Optimizer setting\n",
        "import torch.optim as optim\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters() , lr= 0.001)\n",
        "\n",
        "# 3. 기타 변수들\n",
        "train_info = []\n",
        "val_info = []\n",
        "object_epoch = 175\n",
        "\n",
        "save_path = './ColorizationNetwork'\n",
        "os.makedirs(save_path, exist_ok= True)\n",
        "output_path = os.path.join(save_path, 'colorization_model.tar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdkQLRB1bQVO"
      },
      "source": [
        "from skimage.measure.simple_metrics import compare_psnr\n",
        "\n",
        "def batch_PSNR(img, imclean, data_range):\n",
        "    Img = img.data.cpu().numpy().astype(np.float32)\n",
        "    Iclean = imclean.data.cpu().numpy().astype(np.float32)\n",
        "    PSNR = 0\n",
        "    for i in range(Img.shape[0]):\n",
        "        PSNR += compare_psnr(Iclean[i, :, :, :], Img[i, :, :, :], data_range=data_range)\n",
        "    return (PSNR/Img.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvB-qB--d274"
      },
      "source": [
        "import tqdm \n",
        "\n",
        "def train_1epoch(net, dataloader):\n",
        "  total_loss = 0 # 1 epoch loss\n",
        "  iteration = 1 # iteration number\n",
        "\n",
        "  net.train() # training mode\n",
        "  psnr_train = []\n",
        "\n",
        "  for data in tqdm.auto.tqdm(dataloader):\n",
        "    # 1. 데이터 준비\n",
        "    l = data['l']\n",
        "    ab = data['ab']\n",
        "    hint = data['hint']\n",
        "\n",
        "    # use GPU\n",
        "    if use_cuda: \n",
        "      l = data['l'].to('cuda')\n",
        "      ab = data['ab'].to('cuda')\n",
        "      hint = data['hint'].to('cuda')\n",
        "\n",
        "    gt_image = torch.cat((l, ab), dim=1)\n",
        "    hint_image = torch.cat((l, hint), dim=1)\n",
        "\n",
        "    # 2. gradient 초기화\n",
        "    optimizer.zero_grad() # gradient zero\n",
        "\n",
        "    # 3. 네트워크 결과 얻기 (Forward)\n",
        "    output = net(hint_image)\n",
        "\n",
        "    # 4. loss 얻기\n",
        "    loss = criterion(output, ab)\n",
        "\n",
        "    # 5. gradient 계산\n",
        "    loss.backward()\n",
        "\n",
        "    # 6. gradient 적용\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.detach() # detach -> 계산 그래프의 분리, detach 그래디언트 계산함\n",
        "    iteration += 1\n",
        "\n",
        "    # psnr\n",
        "    output_image = torch.cat((l, output), dim= 1)\n",
        "    psnr = batch_PSNR(gt_image, output_image, 1.)\n",
        "    psnr_train.append(psnr)\n",
        "\n",
        "\n",
        "  mean_psnr = np.mean(psnr_train)\n",
        "  total_loss /= iteration\n",
        "\n",
        "  return mean_psnr, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0bpDIz-jLJo"
      },
      "source": [
        "def validation_1epoch(net, dataloader):\n",
        "  total_loss = 0 # 1 epoch loss\n",
        "  iteration = 1 # iteration number\n",
        "\n",
        "# validation 을 위한 코드 구조\n",
        " # -----------------------------------------------------------------------------\n",
        "  net.eval() # validation mode\n",
        "  psnr_val = []\n",
        "\n",
        "  for data in tqdm.auto.tqdm(dataloader):\n",
        "    # 1. 데이터 준비\n",
        "    l = data['l']\n",
        "    ab = data['ab']\n",
        "    hint = data['hint']\n",
        "\n",
        "    # use GPU\n",
        "    if use_cuda: \n",
        "      l = data['l'].to('cuda')\n",
        "      ab = data['ab'].to('cuda')\n",
        "      hint = data['hint'].to('cuda')\n",
        "\n",
        "    gt_image = torch.cat((l, ab), dim=1)\n",
        "    hint_image = torch.cat((l, hint), dim=1)\n",
        "\n",
        "    # 3. 네트워크 결과 얻기 (Forward)\n",
        "    output = net(hint_image)\n",
        "\n",
        "    # 4. loss 얻기\n",
        "    loss = criterion(output, ab)\n",
        "\n",
        "    # psnr\n",
        "    output_image = torch.cat((l, output), dim= 1)\n",
        "    psnr = batch_PSNR(gt_image, output_image, 1.)\n",
        "    psnr_val.append(psnr)\n",
        "\n",
        " # -----------------------------------------------------------------------------\n",
        "\n",
        "    # detach(): Tensor 기울기 계산 그래프에서 제거\n",
        "    total_loss += loss.detach() # detach -> 계산 그래프의 분리, detach 그래디언트 계산함\n",
        "    iteration += 1\n",
        "  mean_psnr = np.mean(psnr_val)\n",
        "  total_loss /= iteration\n",
        "\n",
        "  return mean_psnr, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5u-IwJNkUas"
      },
      "source": [
        "best_psnr = 0\n",
        "\n",
        "for epoch in range(object_epoch):\n",
        "  train_psnr , train_loss = train_1epoch(net, train_dataloader)\n",
        "  print('[Training] Epoch {}: PSNR: {}, loss: {}'.format(epoch, train_psnr, train_loss))\n",
        "  train_info.append({\n",
        "      'loss': train_loss,\n",
        "      'psnr': train_psnr\n",
        "  }\n",
        "  )\n",
        "\n",
        "  with torch.no_grad(): # gradient 를 계산하지 않겠다 \n",
        "    val_psnr, val_loss = validation_1epoch(net, val_dataloader)\n",
        "  print('[Validation] Epoch {}: psnr: {}, loss: {}'.format(epoch, val_psnr, val_loss))\n",
        "  val_info.append({\n",
        "      'loss': val_loss,\n",
        "      'psnr': val_psnr\n",
        "  }\n",
        "  )\n",
        "\n",
        "  if best_psnr < val_psnr:\n",
        "    best_psnr = val_psnr\n",
        "    torch.save({\n",
        "      'memo': 'Colorization Model',\n",
        "      'psnr': val_psnr,\n",
        "      'loss': val_loss,\n",
        "      'model_weight': net.state_dict()\n",
        "  }, output_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgx70zQMu178"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epoch_axis = np.arange(0, object_epoch)\n",
        "plt.title('PSNR')\n",
        "plt.plot(epoch_axis, [info['psnr'] for info in train_info], epoch_axis, [info['psnr'] for info in val_info], 'r-')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Loss')\n",
        "plt.plot(epoch_axis, [info['loss'] for info in train_info], epoch_axis, [info['loss'] for info in val_info], 'r-')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWcIZafQpH7F"
      },
      "source": [
        "# Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww_Eo8t8pQ_O"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "model_path = os.path.join(save_path, 'colorization_model.tar')\n",
        "state_dict = torch.load(model_path)\n",
        "\n",
        "print(state_dict['memo'])\n",
        "print(state_dict.keys())\n",
        "print(state_dict['loss'])\n",
        "\n",
        "net = UNet().cuda()\n",
        "net.load_state_dict(state_dict['model_weight'], strict= True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bci7HXG01jkM"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "net = UNet().cuda()\n",
        "net.load_state_dict(state_dict['model_weight'], strict= True)\n",
        "\n",
        "# os.makedirs('/content/output', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/output', exist_ok=True)\n",
        "result_path = '/content/drive/MyDrive/output/'\n",
        "\n",
        "def test_1epoch(net, dataloader):\n",
        "  net.eval()\n",
        "\n",
        "  for sample in tqdm.auto.tqdm(dataloader):\n",
        "    if use_cuda:\n",
        "      l = sample[\"l\"].to('cuda')\n",
        "      hint = sample[\"hint\"].to('cuda')\n",
        "      file_name = sample[\"file_name\"]\n",
        "\n",
        "    hint_image = torch.cat((l, hint), dim=1)\n",
        "    output_image = net(hint_image)\n",
        "    output_image = torch.cat((l,output_image),dim=1)\n",
        "\n",
        "    hint_np = tensor2im(hint_image)\n",
        "    output_np = tensor2im(output_image)\n",
        "\n",
        "    hint_bgr = cv2.cvtColor(hint_np, cv2.COLOR_LAB2BGR)\n",
        "    output_bgr = cv2.cvtColor(output_np, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # cv2_imshow(hint_bgr)\n",
        "    # cv2_imshow(output_bgr)\n",
        "\n",
        "    cv2.imwrite(result_path+file_name[0], output_bgr)\n",
        "\n",
        "\n",
        "res = test_1epoch(net, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVyLALF3m-Se"
      },
      "source": [
        "sorted(os.listdir('/content/drive/MyDrive/output'), key=lambda x: int((x.split('_')[1]).split('.')[0]))\n",
        "\n",
        "root = '/content/drive/MyDrive/output/'\n",
        "re = os.listdir(root)\n",
        "print(len(re))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}